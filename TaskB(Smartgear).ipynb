{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "75d00d6b-b99b-4dbc-acad-340c7e062ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-------+----------+--------+---------+\n|OrderID| OrderDate|Region|StoreID|   Product|Quantity|UnitPrice|\n+-------+----------+------+-------+----------+--------+---------+\n|   1001|2025-03-14|  East|    115|Headphones|       3|    81.65|\n|   1002|2025-02-19|  West|    110|Smartwatch|       3|    242.5|\n|   1003|2025-03-17|  West|    113|   Printer|       2|   152.59|\n|   1004|2025-01-05|  West|    118|    Camera|       4|    463.4|\n|   1005|2025-01-07|  West|    110|    Tablet|       3|   370.01|\n+-------+----------+------+-------+----------+--------+---------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# SAS token\n",
    "sas_token = \"sp=racwdli&st=2025-09-03T05:45:03Z&se=2025-09-03T14:00:03Z&spr=https&sv=2024-11-04&sr=c&sig=lC3dWqTbd47C%2FkbpAwPtRsHX1PV73R0Sbq3Vi6CEtQw%3D\"\n",
    "\n",
    "storage_account_name = \"rgassmartgear\"\n",
    "container_name = \"raw\"\n",
    "file_name = \"smartgear_sales.csv\"\n",
    "\n",
    "# Configure Hadoop to use SAS\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\",\n",
    "  sas_token\n",
    ")\n",
    "\n",
    "# Now build the path (without token in URL this time)\n",
    "csv_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{file_name}\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.option(\"header\", \"true\").csv(csv_path)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92da444d-8f29-4292-b7b7-4e5e90fd55ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658a4164-4aa3-4e8b-b752-5017fde4e83c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Quantity\", df[\"Quantity\"].cast(\"int\")) #typecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a70e372-202d-48f1-9adb-a2dad8f881b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n|Region|Total_Quantity|\n+------+--------------+\n| South|           740|\n|  East|           719|\n|  West|           694|\n| North|           848|\n+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregate total quantity by region\n",
    "region_qty_df = df.groupBy(\"Region\").agg(_sum(\"Quantity\").alias(\"Total_Quantity\"))\n",
    "\n",
    "region_qty_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104730d1-7843-4826-a801-9701d98266d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ region_qty written successfully to Blob Storage as Parquet\n"
     ]
    }
   ],
   "source": [
    "# Apply SAS for both blob + dfs endpoints\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\",\n",
    "    sas_token\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{container_name}.{storage_account_name}.dfs.core.windows.net\",\n",
    "    sas_token\n",
    ")\n",
    "\n",
    "\n",
    "# Output path\n",
    "parquet_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/region_qty\"\n",
    "\n",
    "# Force overwrite + Parquet\n",
    "(region_qty_df\n",
    "    .coalesce(1)                 # optional → single file\n",
    "    .write.mode(\"overwrite\")     # clean old folder\n",
    "    .option(\"overwriteSchema\", \"true\")  # avoid schema mismatch\n",
    "    .format(\"parquet\")           # force Parquet (not Delta)\n",
    "    .save(parquet_path))\n",
    "\n",
    "print(\"✅ region_qty written successfully to Blob Storage as Parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Smartgear",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}